{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Mixed precision compatibility check (mixed_float16): OK\n",
      "Your GPU will likely run quickly with dtype policy mixed_float16 as it has compute capability of at least 7.0. Your GPU: NVIDIA GeForce RTX 3060 Laptop GPU, compute capability 8.6\n",
      "Compute dtype: float16\n",
      "Variable dtype: float32\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, mixed_precision, optimizers\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "\n",
    "import keras_tuner as kt\n",
    "\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)\n",
    "\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "\n",
    "policy = mixed_precision.Policy('mixed_float16')\n",
    "mixed_precision.set_global_policy(policy)\n",
    "print('Compute dtype:', policy.compute_dtype)\n",
    "print('Variable dtype:', policy.variable_dtype)\n",
    "\n",
    "root_dir = \"E:\\\\Repositories\\\\personal-projects\\\\ai-ml-projects\\\\bmi-classification\"\n",
    "os.chdir(root_dir)\n",
    "\n",
    "data_path = os.path.join(root_dir, \"data\", \"processed\")\n",
    "log_dir = os.path.join(root_dir, \"logs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Gender    Height    Weight  Index_0  Index_1  Index_2  Index_3  Index_4  \\\n",
      "0  1.011174  0.235303 -0.310062      0.0      0.0      0.0      0.0      1.0   \n",
      "1  1.011174  1.147330 -0.588376      0.0      0.0      1.0      0.0      0.0   \n",
      "2 -0.988950  0.904123  0.122870      0.0      0.0      0.0      0.0      1.0   \n",
      "3 -0.988950  1.512141 -0.062672      0.0      0.0      0.0      1.0      0.0   \n",
      "4  1.011174 -1.284742 -1.392394      0.0      0.0      0.0      1.0      0.0   \n",
      "\n",
      "   Index_5  \n",
      "0      0.0  \n",
      "1      0.0  \n",
      "2      0.0  \n",
      "3      0.0  \n",
      "4      0.0  \n"
     ]
    }
   ],
   "source": [
    "class Dataset:\n",
    "    def __init__(self, data_path):\n",
    "        self.data_path = data_path\n",
    "\n",
    "    def load_data(self):\n",
    "        data_frames = []\n",
    "        for file in os.listdir(self.data_path):\n",
    "            if file.endswith(\".csv\"):\n",
    "                data_buffer = pd.read_csv(os.path.join(self.data_path, file))\n",
    "                data_frames.append(data_buffer)\n",
    "        \n",
    "        if not data_frames:\n",
    "            print(\"No CSV files found in the specified directory.\")\n",
    "            return None\n",
    "        \n",
    "        # Combine all data frames into a single data frame\n",
    "        combined_data = pd.concat(data_frames, ignore_index=True)\n",
    "        \n",
    "        return combined_data\n",
    "\n",
    "dataset = Dataset(data_path)\n",
    "data = dataset.load_data()\n",
    "\n",
    "if data is not None:\n",
    "    print(data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure 'X' contains all feature columns except the one-hot encoded 'Index' columns\n",
    "X = data.drop(columns=data.columns[-6:]).astype(np.float32)\n",
    "y = data[data.columns[-6:]].astype(np.float32)\n",
    "\n",
    "X_train, X_subset, y_train, y_subset = train_test_split(X, y, test_size=0.2, random_state=seed)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_subset, y_subset, test_size=0.5, random_state=seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Gender</th>\n",
       "      <th>Height</th>\n",
       "      <th>Weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1.011173</td>\n",
       "      <td>1.512141</td>\n",
       "      <td>-0.773919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>334</th>\n",
       "      <td>-0.988950</td>\n",
       "      <td>-0.798328</td>\n",
       "      <td>-1.547013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>244</th>\n",
       "      <td>-0.988950</td>\n",
       "      <td>0.782519</td>\n",
       "      <td>-1.732556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>678</th>\n",
       "      <td>-0.988950</td>\n",
       "      <td>1.329736</td>\n",
       "      <td>1.019660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>306</th>\n",
       "      <td>1.011173</td>\n",
       "      <td>0.782519</td>\n",
       "      <td>1.452593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>1.011173</td>\n",
       "      <td>-0.251112</td>\n",
       "      <td>1.669059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>270</th>\n",
       "      <td>1.011173</td>\n",
       "      <td>-1.527950</td>\n",
       "      <td>0.741346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>860</th>\n",
       "      <td>-0.988950</td>\n",
       "      <td>1.086528</td>\n",
       "      <td>-0.804843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>435</th>\n",
       "      <td>1.011173</td>\n",
       "      <td>-0.129508</td>\n",
       "      <td>1.607211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>1.011173</td>\n",
       "      <td>-0.555121</td>\n",
       "      <td>1.514440</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>720 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Gender    Height    Weight\n",
       "10   1.011173  1.512141 -0.773919\n",
       "334 -0.988950 -0.798328 -1.547013\n",
       "244 -0.988950  0.782519 -1.732556\n",
       "678 -0.988950  1.329736  1.019660\n",
       "306  1.011173  0.782519  1.452593\n",
       "..        ...       ...       ...\n",
       "106  1.011173 -0.251112  1.669059\n",
       "270  1.011173 -1.527950  0.741346\n",
       "860 -0.988950  1.086528 -0.804843\n",
       "435  1.011173 -0.129508  1.607211\n",
       "102  1.011173 -0.555121  1.514440\n",
       "\n",
       "[720 rows x 3 columns]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss_and_accuracy(history):\n",
    "    plt.figure(figsize=(12, 5))\n",
    "\n",
    "    # Plotting Loss\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['loss'], label='Train Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.title('Loss Over Epochs')\n",
    "\n",
    "    # Plotting Accuracy\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['mae'], label='Train mae')\n",
    "    plt.plot(history.history['val_mae'], label='Validation mae')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('mae')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.title('mae Over Epochs')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HyperModel(kt.HyperModel):\n",
    "    def build(self, hp):\n",
    "        model = keras.Sequential()\n",
    "        model.add(layers.Input(shape=(X_train.shape[1],)))\n",
    "        \n",
    "        for i in range(hp.Int('num_layers', 2, 5)):\n",
    "            model.add(layers.Dense(units=hp.Int(f'units_{i}', min_value=32, max_value=512, step=32),\n",
    "                                   activation='relu'))\n",
    "            if hp.Boolean(f'use_batchnorm_{i}'):\n",
    "                model.add(layers.BatchNormalization())\n",
    "            \n",
    "        model.add(layers.Dropout(hp.Float('dropout', 0.1, 0.5, step=0.1)))\n",
    "        model.add(layers.Dense(y_train.shape[1], activation='softmax'))\n",
    "\n",
    "        optimizer_choice = hp.Choice('optimizer', values=['adam', 'sgd', 'rmsprop'])\n",
    "        lr = hp.Float('learning_rate', 1e-7, 1e-3, sampling='log')\n",
    "\n",
    "        if optimizer_choice == 'adam':\n",
    "            optimizer = optimizers.Adam(learning_rate=lr)\n",
    "        elif optimizer_choice == 'sgd':\n",
    "            optimizer = optimizers.SGD(learning_rate=lr)\n",
    "        else:\n",
    "            optimizer = optimizers.RMSprop(learning_rate=lr)\n",
    "        \n",
    "        model.compile(optimizer=optimizer,\n",
    "                      loss='categorical_crossentropy',\n",
    "                      metrics=['accuracy'])\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    def fit(self, hp, model, *args, **kwargs):\n",
    "        return model.fit(\n",
    "            *args, \n",
    "            batch_size=hp.Int('batch_size', 32, 128, step=32),\n",
    "            shuffle=hp.Boolean('shuffle'),\n",
    "            **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reloading Tuner from logs/keras_tuner\\BMI_Classification\\tuner0.json\n"
     ]
    }
   ],
   "source": [
    "# Function to clear GPU memory\n",
    "def clear_gpu_memory():\n",
    "    keras.backend.clear_session()\n",
    "    \n",
    "# Custom callback to clear session after each trial\n",
    "class ClearMemory(tf.keras.callbacks.Callback):\n",
    "    def on_train_end(self, logs=None):\n",
    "        clear_gpu_memory()\n",
    "\n",
    "# Define a Keras Tuner tuner\n",
    "tuner = kt.BayesianOptimization(\n",
    "    HyperModel(),\n",
    "    objective='val_accuracy',\n",
    "    max_trials=250,\n",
    "    executions_per_trial=1,\n",
    "    directory='logs/keras_tuner',\n",
    "    project_name='BMI_Classification'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search space summary\n",
      "Default search space size: 16\n",
      "num_layers (Int)\n",
      "{'default': None, 'conditions': [], 'min_value': 2, 'max_value': 5, 'step': 1, 'sampling': 'linear'}\n",
      "units_0 (Int)\n",
      "{'default': None, 'conditions': [], 'min_value': 32, 'max_value': 512, 'step': 32, 'sampling': 'linear'}\n",
      "use_batchnorm_0 (Boolean)\n",
      "{'default': False, 'conditions': []}\n",
      "units_1 (Int)\n",
      "{'default': None, 'conditions': [], 'min_value': 32, 'max_value': 512, 'step': 32, 'sampling': 'linear'}\n",
      "use_batchnorm_1 (Boolean)\n",
      "{'default': False, 'conditions': []}\n",
      "dropout (Float)\n",
      "{'default': 0.1, 'conditions': [], 'min_value': 0.1, 'max_value': 0.5, 'step': 0.1, 'sampling': 'linear'}\n",
      "optimizer (Choice)\n",
      "{'default': 'adam', 'conditions': [], 'values': ['adam', 'sgd', 'rmsprop'], 'ordered': False}\n",
      "learning_rate (Float)\n",
      "{'default': 1e-07, 'conditions': [], 'min_value': 1e-07, 'max_value': 0.001, 'step': None, 'sampling': 'log'}\n",
      "units_2 (Int)\n",
      "{'default': None, 'conditions': [], 'min_value': 32, 'max_value': 512, 'step': 32, 'sampling': 'linear'}\n",
      "use_batchnorm_2 (Boolean)\n",
      "{'default': False, 'conditions': []}\n",
      "units_3 (Int)\n",
      "{'default': None, 'conditions': [], 'min_value': 32, 'max_value': 512, 'step': 32, 'sampling': 'linear'}\n",
      "use_batchnorm_3 (Boolean)\n",
      "{'default': False, 'conditions': []}\n",
      "batch_size (Int)\n",
      "{'default': None, 'conditions': [], 'min_value': 32, 'max_value': 128, 'step': 32, 'sampling': 'linear'}\n",
      "shuffle (Boolean)\n",
      "{'default': False, 'conditions': []}\n",
      "units_4 (Int)\n",
      "{'default': None, 'conditions': [], 'min_value': 32, 'max_value': 512, 'step': 32, 'sampling': 'linear'}\n",
      "use_batchnorm_4 (Boolean)\n",
      "{'default': False, 'conditions': []}\n"
     ]
    }
   ],
   "source": [
    "tuner.search_space_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 250 Complete [00h 00m 16s]\n",
      "val_accuracy: 0.9222221970558167\n",
      "\n",
      "Best val_accuracy So Far: 0.9777777791023254\n",
      "Total elapsed time: 00h 58m 06s\n"
     ]
    }
   ],
   "source": [
    "# Perform the hyperparameter search\n",
    "tuner.search(X_train, y_train, epochs=50, validation_data=(X_val, y_val), callbacks=[ClearMemory()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results summary\n",
      "Results in logs/keras_tuner\\BMI_Classification\n",
      "Showing 10 best trials\n",
      "Objective(name=\"val_accuracy\", direction=\"max\")\n",
      "\n",
      "Trial 032 summary\n",
      "Hyperparameters:\n",
      "num_layers: 5\n",
      "units_0: 480\n",
      "use_batchnorm_0: False\n",
      "units_1: 416\n",
      "use_batchnorm_1: False\n",
      "dropout: 0.2\n",
      "optimizer: adam\n",
      "learning_rate: 0.0006250560171450277\n",
      "units_2: 64\n",
      "use_batchnorm_2: False\n",
      "units_3: 480\n",
      "use_batchnorm_3: False\n",
      "batch_size: 128\n",
      "shuffle: False\n",
      "units_4: 352\n",
      "use_batchnorm_4: False\n",
      "Score: 0.9777777791023254\n",
      "\n",
      "Trial 129 summary\n",
      "Hyperparameters:\n",
      "num_layers: 2\n",
      "units_0: 512\n",
      "use_batchnorm_0: False\n",
      "units_1: 512\n",
      "use_batchnorm_1: True\n",
      "dropout: 0.1\n",
      "optimizer: adam\n",
      "learning_rate: 0.001\n",
      "units_2: 512\n",
      "use_batchnorm_2: False\n",
      "units_3: 32\n",
      "use_batchnorm_3: False\n",
      "batch_size: 32\n",
      "shuffle: False\n",
      "units_4: 32\n",
      "use_batchnorm_4: False\n",
      "Score: 0.9777777791023254\n",
      "\n",
      "Trial 211 summary\n",
      "Hyperparameters:\n",
      "num_layers: 5\n",
      "units_0: 512\n",
      "use_batchnorm_0: False\n",
      "units_1: 512\n",
      "use_batchnorm_1: False\n",
      "dropout: 0.1\n",
      "optimizer: adam\n",
      "learning_rate: 0.001\n",
      "units_2: 32\n",
      "use_batchnorm_2: False\n",
      "units_3: 512\n",
      "use_batchnorm_3: False\n",
      "batch_size: 128\n",
      "shuffle: False\n",
      "units_4: 384\n",
      "use_batchnorm_4: False\n",
      "Score: 0.9777777791023254\n",
      "\n",
      "Trial 059 summary\n",
      "Hyperparameters:\n",
      "num_layers: 3\n",
      "units_0: 480\n",
      "use_batchnorm_0: False\n",
      "units_1: 416\n",
      "use_batchnorm_1: True\n",
      "dropout: 0.30000000000000004\n",
      "optimizer: adam\n",
      "learning_rate: 0.001\n",
      "units_2: 512\n",
      "use_batchnorm_2: False\n",
      "units_3: 192\n",
      "use_batchnorm_3: True\n",
      "batch_size: 32\n",
      "shuffle: False\n",
      "units_4: 32\n",
      "use_batchnorm_4: False\n",
      "Score: 0.9666666388511658\n",
      "\n",
      "Trial 060 summary\n",
      "Hyperparameters:\n",
      "num_layers: 5\n",
      "units_0: 512\n",
      "use_batchnorm_0: False\n",
      "units_1: 160\n",
      "use_batchnorm_1: False\n",
      "dropout: 0.30000000000000004\n",
      "optimizer: adam\n",
      "learning_rate: 0.001\n",
      "units_2: 320\n",
      "use_batchnorm_2: False\n",
      "units_3: 512\n",
      "use_batchnorm_3: True\n",
      "batch_size: 64\n",
      "shuffle: False\n",
      "units_4: 256\n",
      "use_batchnorm_4: False\n",
      "Score: 0.9666666388511658\n",
      "\n",
      "Trial 066 summary\n",
      "Hyperparameters:\n",
      "num_layers: 5\n",
      "units_0: 512\n",
      "use_batchnorm_0: False\n",
      "units_1: 512\n",
      "use_batchnorm_1: True\n",
      "dropout: 0.1\n",
      "optimizer: adam\n",
      "learning_rate: 0.001\n",
      "units_2: 512\n",
      "use_batchnorm_2: False\n",
      "units_3: 96\n",
      "use_batchnorm_3: True\n",
      "batch_size: 32\n",
      "shuffle: False\n",
      "units_4: 32\n",
      "use_batchnorm_4: False\n",
      "Score: 0.9666666388511658\n",
      "\n",
      "Trial 069 summary\n",
      "Hyperparameters:\n",
      "num_layers: 3\n",
      "units_0: 32\n",
      "use_batchnorm_0: True\n",
      "units_1: 416\n",
      "use_batchnorm_1: True\n",
      "dropout: 0.5\n",
      "optimizer: adam\n",
      "learning_rate: 0.001\n",
      "units_2: 512\n",
      "use_batchnorm_2: False\n",
      "units_3: 32\n",
      "use_batchnorm_3: True\n",
      "batch_size: 32\n",
      "shuffle: False\n",
      "units_4: 32\n",
      "use_batchnorm_4: False\n",
      "Score: 0.9666666388511658\n",
      "\n",
      "Trial 102 summary\n",
      "Hyperparameters:\n",
      "num_layers: 5\n",
      "units_0: 512\n",
      "use_batchnorm_0: False\n",
      "units_1: 512\n",
      "use_batchnorm_1: False\n",
      "dropout: 0.1\n",
      "optimizer: adam\n",
      "learning_rate: 0.001\n",
      "units_2: 32\n",
      "use_batchnorm_2: False\n",
      "units_3: 512\n",
      "use_batchnorm_3: True\n",
      "batch_size: 128\n",
      "shuffle: False\n",
      "units_4: 512\n",
      "use_batchnorm_4: False\n",
      "Score: 0.9666666388511658\n",
      "\n",
      "Trial 105 summary\n",
      "Hyperparameters:\n",
      "num_layers: 4\n",
      "units_0: 32\n",
      "use_batchnorm_0: False\n",
      "units_1: 512\n",
      "use_batchnorm_1: True\n",
      "dropout: 0.1\n",
      "optimizer: rmsprop\n",
      "learning_rate: 0.001\n",
      "units_2: 32\n",
      "use_batchnorm_2: True\n",
      "units_3: 32\n",
      "use_batchnorm_3: True\n",
      "batch_size: 32\n",
      "shuffle: True\n",
      "units_4: 512\n",
      "use_batchnorm_4: False\n",
      "Score: 0.9666666388511658\n",
      "\n",
      "Trial 120 summary\n",
      "Hyperparameters:\n",
      "num_layers: 5\n",
      "units_0: 32\n",
      "use_batchnorm_0: True\n",
      "units_1: 512\n",
      "use_batchnorm_1: False\n",
      "dropout: 0.1\n",
      "optimizer: rmsprop\n",
      "learning_rate: 0.001\n",
      "units_2: 512\n",
      "use_batchnorm_2: True\n",
      "units_3: 32\n",
      "use_batchnorm_3: True\n",
      "batch_size: 32\n",
      "shuffle: True\n",
      "units_4: 512\n",
      "use_batchnorm_4: False\n",
      "Score: 0.9666666388511658\n"
     ]
    }
   ],
   "source": [
    "tuner.results_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The optimal number of layers is 2.\n",
      "The optimal number of units is [512, 512].\n",
      "The optimal dropout rate is 0.1.\n",
      "The optimal learning rate for the optimizer is 0.001.\n",
      "The optimal optimizer is adam.\n",
      "The optimal batch size is 32.\n",
      "The optimal shuffle value is False.\n",
      "\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 512)               2048      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 512)               262656    \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 512)              2048      \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 512)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 6)                 3078      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 269,830\n",
      "Trainable params: 268,806\n",
      "Non-trainable params: 1,024\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Retrieve the best hyperparameters\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials=10)[1]\n",
    "\n",
    "# Print best hyperparameters\n",
    "print(f\"\"\"\n",
    "The optimal number of layers is {best_hps.get('num_layers')}.\n",
    "The optimal number of units is {[best_hps.get(f'units_{i}') for i in range(best_hps.get('num_layers'))]}.\n",
    "The optimal dropout rate is {best_hps.get('dropout')}.\n",
    "The optimal learning rate for the optimizer is {best_hps.get('learning_rate')}.\n",
    "The optimal optimizer is {best_hps.get('optimizer')}.\n",
    "The optimal batch size is {best_hps.get('batch_size')}.\n",
    "The optimal shuffle value is {best_hps.get('shuffle')}.\n",
    "\"\"\")\n",
    "\n",
    "# Build the model with the optimal hyperparameters and train it on the data\n",
    "model = tuner.hypermodel.build(best_hps)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "23/23 [==============================] - 1s 12ms/step - loss: 0.8507 - accuracy: 0.6958 - val_loss: 1.3734 - val_accuracy: 0.7222\n",
      "Epoch 2/50\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.4753 - accuracy: 0.8194 - val_loss: 1.2795 - val_accuracy: 0.7333\n",
      "Epoch 3/50\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.4280 - accuracy: 0.8306 - val_loss: 1.2084 - val_accuracy: 0.7667\n",
      "Epoch 4/50\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.3788 - accuracy: 0.8472 - val_loss: 1.1181 - val_accuracy: 0.8444\n",
      "Epoch 5/50\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.3718 - accuracy: 0.8653 - val_loss: 1.0515 - val_accuracy: 0.8333\n",
      "Epoch 6/50\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.3521 - accuracy: 0.8583 - val_loss: 0.9646 - val_accuracy: 0.7778\n",
      "Epoch 7/50\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.3084 - accuracy: 0.8764 - val_loss: 0.8891 - val_accuracy: 0.7889\n",
      "Epoch 8/50\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.3486 - accuracy: 0.8722 - val_loss: 0.8106 - val_accuracy: 0.8889\n",
      "Epoch 9/50\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.2893 - accuracy: 0.8819 - val_loss: 0.7500 - val_accuracy: 0.8000\n",
      "Epoch 10/50\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.3182 - accuracy: 0.8861 - val_loss: 0.6671 - val_accuracy: 0.8444\n",
      "Epoch 11/50\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.2955 - accuracy: 0.8986 - val_loss: 0.5872 - val_accuracy: 0.9000\n",
      "Epoch 12/50\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.3022 - accuracy: 0.8889 - val_loss: 0.5305 - val_accuracy: 0.8667\n",
      "Epoch 13/50\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.2875 - accuracy: 0.8875 - val_loss: 0.4879 - val_accuracy: 0.8222\n",
      "Epoch 14/50\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.2498 - accuracy: 0.9014 - val_loss: 0.4223 - val_accuracy: 0.8889\n",
      "Epoch 15/50\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.2586 - accuracy: 0.9069 - val_loss: 0.3977 - val_accuracy: 0.8889\n",
      "Epoch 16/50\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.2756 - accuracy: 0.8819 - val_loss: 0.3987 - val_accuracy: 0.8889\n",
      "Epoch 17/50\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 0.2817 - accuracy: 0.9028 - val_loss: 0.3361 - val_accuracy: 0.9000\n",
      "Epoch 18/50\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.2500 - accuracy: 0.9125 - val_loss: 0.3864 - val_accuracy: 0.8778\n",
      "Epoch 19/50\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 0.2734 - accuracy: 0.8917 - val_loss: 0.3622 - val_accuracy: 0.8333\n",
      "Epoch 20/50\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.2598 - accuracy: 0.8792 - val_loss: 0.2747 - val_accuracy: 0.9000\n",
      "Epoch 21/50\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 0.2602 - accuracy: 0.8917 - val_loss: 0.2735 - val_accuracy: 0.9111\n",
      "Epoch 22/50\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 0.2400 - accuracy: 0.9194 - val_loss: 0.2478 - val_accuracy: 0.9333\n",
      "Epoch 23/50\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 0.2115 - accuracy: 0.9111 - val_loss: 0.2458 - val_accuracy: 0.9222\n",
      "Epoch 24/50\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 0.2057 - accuracy: 0.9153 - val_loss: 0.2553 - val_accuracy: 0.9111\n",
      "Epoch 25/50\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 0.2151 - accuracy: 0.9167 - val_loss: 0.2403 - val_accuracy: 0.9111\n",
      "Epoch 26/50\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.2040 - accuracy: 0.9250 - val_loss: 0.2057 - val_accuracy: 0.9222\n",
      "Epoch 27/50\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 0.2293 - accuracy: 0.9153 - val_loss: 0.3390 - val_accuracy: 0.8556\n",
      "Epoch 28/50\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.1984 - accuracy: 0.9264 - val_loss: 0.1683 - val_accuracy: 0.9222\n",
      "Epoch 29/50\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.2157 - accuracy: 0.9181 - val_loss: 0.2190 - val_accuracy: 0.8889\n",
      "Epoch 30/50\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.2345 - accuracy: 0.9069 - val_loss: 0.3047 - val_accuracy: 0.8667\n",
      "Epoch 31/50\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.2291 - accuracy: 0.9222 - val_loss: 0.2276 - val_accuracy: 0.9111\n",
      "Epoch 32/50\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 0.2023 - accuracy: 0.9250 - val_loss: 0.2071 - val_accuracy: 0.9333\n",
      "Epoch 33/50\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 0.2208 - accuracy: 0.9153 - val_loss: 0.2733 - val_accuracy: 0.8778\n",
      "Epoch 34/50\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 0.2054 - accuracy: 0.9153 - val_loss: 0.2315 - val_accuracy: 0.9222\n",
      "Epoch 35/50\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 0.1835 - accuracy: 0.9444 - val_loss: 0.1950 - val_accuracy: 0.9000\n",
      "Epoch 36/50\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 0.1969 - accuracy: 0.9278 - val_loss: 0.1618 - val_accuracy: 0.9556\n",
      "Epoch 37/50\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 0.1979 - accuracy: 0.9181 - val_loss: 0.2478 - val_accuracy: 0.8889\n",
      "Epoch 38/50\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 0.2179 - accuracy: 0.9167 - val_loss: 0.2638 - val_accuracy: 0.8778\n",
      "Epoch 39/50\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 0.2012 - accuracy: 0.9250 - val_loss: 0.1762 - val_accuracy: 0.9556\n",
      "Epoch 40/50\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 0.1775 - accuracy: 0.9236 - val_loss: 0.1861 - val_accuracy: 0.9333\n",
      "Epoch 41/50\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 0.1804 - accuracy: 0.9319 - val_loss: 0.2005 - val_accuracy: 0.9222\n",
      "Epoch 42/50\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 0.1666 - accuracy: 0.9389 - val_loss: 0.1848 - val_accuracy: 0.9111\n",
      "Epoch 43/50\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 0.1696 - accuracy: 0.9361 - val_loss: 0.2128 - val_accuracy: 0.9000\n",
      "Epoch 44/50\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 0.2021 - accuracy: 0.9250 - val_loss: 0.2079 - val_accuracy: 0.9444\n",
      "Epoch 45/50\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 0.2024 - accuracy: 0.9167 - val_loss: 0.2062 - val_accuracy: 0.9222\n",
      "Epoch 46/50\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 0.1818 - accuracy: 0.9375 - val_loss: 0.2029 - val_accuracy: 0.9333\n",
      "Epoch 47/50\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 0.1894 - accuracy: 0.9250 - val_loss: 0.1814 - val_accuracy: 0.9333\n",
      "Epoch 48/50\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 0.1443 - accuracy: 0.9444 - val_loss: 0.3209 - val_accuracy: 0.8889\n",
      "Epoch 49/50\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 0.2195 - accuracy: 0.9236 - val_loss: 0.2051 - val_accuracy: 0.9222\n",
      "Epoch 50/50\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.1557 - accuracy: 0.9347 - val_loss: 0.1737 - val_accuracy: 0.9000\n",
      "3/3 - 0s - loss: 0.1631 - accuracy: 0.9444 - 52ms/epoch - 17ms/step\n",
      "Validation accuracy: 0.9444444179534912\n"
     ]
    }
   ],
   "source": [
    "mixed_precision.set_global_policy('float32')\n",
    "\n",
    "# Build the model with the best hyperparameters and train it\n",
    "model = tuner.hypermodel.build(best_hps)\n",
    "history = model.fit(X_train, y_train, epochs=50, validation_data=(X_val, y_val), callbacks=[ClearMemory()], batch_size=best_hps.get('batch_size'))\n",
    "\n",
    "# Evaluate the model\n",
    "val_loss, val_accuracy = model.evaluate(X_test, y_test, verbose=2)\n",
    "print(f'Validation accuracy: {val_accuracy}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
